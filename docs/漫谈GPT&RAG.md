## ChatGPT漫谈

由于其超大的参数量以及训练方式，大语言模型具备了更强的语义理解能力，一方面，它现在成为了对话机器人的最佳实践方式，另一方面，在基本的NLP任务中表现出了极强的能力。

现阶段，有两种成本较低的方式可以提高大模型在特定任务或特定领域的能力

1. 调优Prompt，这是最具性价比的调整方式。
2. SFT。针对特定任务，特定领域，对大模型微调。

虽然大模型的能力十分惊艳，但是它也有一些问题亟待解决。首先是大模型生成的结果并不置信（幻觉Hallucination），其次，如果没有对齐人类价值观（Alignment），大模型会生成一些有害的回答，然后是安全问题：比如大模型如何避免被劫持，生成有害的回答，以及生成数据的版权问题。

### Hallucination

定义：当模型生成的文本不遵循原文（Faithfulness）或者不符合事实（Factualness），我们就可以认为模型出现了幻觉的问题。

**什么是Faithfulness and Factualness：**

- **Faithfulness**：是否遵循input content；
- **Factualness**：是否符合世界知识；

众所周知，大型语言模型是基于上下文来输出下一个字符的最大概率预测的模型，它并不能百分之百地保证输出的准确性。在使用ChatGPT时，通常有两种简单的验证方式。第一种是通过询问具备先验知识的问题来评估其可信度，第二种是检查它提供的解决方案是否可行，因为这样可以验证答案的正确性。当然，我们对于ChatGPT的准确性往往是基于主观的定性分析。

然而，在涉及未知领域的问题时，难以确定ChatGPT的回答是否正确，因为它是一个概率模型，存在误差的可能。如果让ChatGPT在回答问题时附上其回答的参考依据，或者提供详细的解答思路，这样用户可以追溯其答案的来源或者分析，从而在未知领域中也能够进行定性分析来评估其准确性。

### Safety&alignment

对于"对齐（Alignment）"这一概念，尚没有一个确切的定义，通常它指的是确保大型人工智能模型与人类的意图相一致。对齐问题在当前的背景下显得尤为重要，因为它有助于避免大型模型产生有害的回答。从长远来看，未能对齐的人工智能模型可能具有危险性，甚至可能导致人类的绝灭或不可逆转的全球灾难。

其中，对齐人类意图是对齐的意义，但是究竟什么是人类意图？一般论文中提及的较多的是人类的价值观，但是价值观和社会形态是紧密相关的，不同社会形态下的人们可能拥有有重叠的价值观，但如何在每种社会形态中完全匹配这些价值观，是一个相当复杂的挑战。当前看来，对齐问题已经超越了纯粹的技术和科学范畴，也牵涉到了社会学、政治学、经济学等多个领域。在这一领域中，对齐方法被视为社会技术系统（socio-technical system）的一个典型应用，是一个非常前沿的研究方向。

在《AGI safety from first principles》提出了两种对齐AGI的方法：最小主义和最大主义。

最小主义：

- **核心思想**：这种方法专注于避免灾难性的结果，而不是使AI系统采纳或顺从一个特定的、全面的价值观体系。
- **目标**：目的是确保AI的行为与操作者的意图对齐。换句话说，AI试图做操作者希望它做的事情。
- **实现方式**：在这种方法中，AI的设计和训练更多地是关注于理解和执行具体的、由人类设定的任务或目标，而非试图理解或采纳更广泛的价值观或道德理论。
- **优势**：由于其简单明了的目标，这种方法在技术实现上相对更易于管理和控制。
- **局限性**：它可能无法处理复杂的道德决策或全球性问题，因为它不涉及广泛的价值观或道德判断。

最大主义：

- **核心思想**：这种方法试图使AI系统采纳或顺从一个特定的、全面的价值观体系，如某种道德理论、全球民主共识或决策程序。
- **目标**：不仅仅是避免灾难性结果，而是积极地使AI行为与某一集合的道德或价值观相符合。
- **实现方式**：这涉及到更复杂的AI设计，需要AI不仅理解具体任务，还要理解和内化更广泛的价值观和伦理原则。
- **优势**：可能更适合处理复杂的、涉及广泛价值观和道德考量的问题。
- **局限性**：技术上更具挑战性，而且在全球范围内实现价值观对齐存在显著的伦理和政治问题。

OpenAI在最近superalignment提及，他们想建立一个和人类水平相当的自动对齐的系统，主要包含3部分：开发可扩展的训练方法；验证模型的效果；对对齐pipeline进行压力测试。

### 对劳动力市场的影响

随着AGI的兴起，尤其以ChatGPT为代表，已经对人类社会的就业产生了冲击。它必然会提高社会的生产力，变革社会中的生产关系，改变现有工作模式。

当前阶段的Diffusion模型已经能够生成非常逼真的图像，这对于流程化的绘画工作产生了重大影响。随着GPT-5、GPT-6和GPT-7等模型的问世，当它们的编码能力足以替代程序员时，是否会有大量程序员被取代呢？我们需要警惕，并且保持警觉，因为ChatGPT的出现也对我们提出了更高的要求。技术在快速更迭，时代在快速的变化，这也要求我们需要调整心态顺应社会的发展。

这里搬运下邱泽奇老师（北京大学社会学系教授，教育部长江学者特聘教授）在北京大学举办的“潘多拉的魔盒还是文明利器——大文明视野中的ChatGPT反思 跨学科研讨会”的圆桌内容。

> 归纳 ChatGPT 所列举的能力，笔者认为，有三类能力是ChatGPT暂时不具备且需要人类自己掌握的。
> 
> 
> **第一，在与人的互动中驾驭情绪的能力。**驾驭情绪是人类的特征之一，许多动物也有情绪，可是能驾驭情绪的， 尤其是在变化的场景中驾驭情绪的，只有人类。在赫克曼的情商说中，驾驭情绪是人类情商重要的组成部分。人类需要驾驭情绪是因为人类职业产出的本质也是情绪的，人类追求的快乐或幸福感， 在本质上是人类的积极情绪。换句话说， 如果说ChatGPT的目标是高效（不一定高质量）生产，人类的目标便是通过高效且高质量地生产，以体现人的社会价值，进而让人产生积极情绪，呈现人的价值和意义。
> 
> **第二，在不断变化的世界里适应变化的能力。**这包括学习能力、批判能力、 创造能力、解决能力等。伴随着现代化进程的推进，人类面临的自然环境和社会环境因技术创新的发展而在加速变化。一个直观的事实是，在过去 100年里，人类职业技能的生命周期在加速缩短，且不得不通过增强人在变化世界中的适应能力来维系生命的社会价值和意义。**如果我们也把ChatGPT的挑战理解为变化世界的一部分，则适应能力才是人应对 AIGC 甚至未来强人工智能冲击的基本能力。** 
> 
> **第三，在技术不断触及人类伦理边界时，坚守人类价值观和设定人类伦理边界的能力。**人类的价值观和伦理边界始终随着人类经济和社会的发展在变化。变化的方向从宗教主导、社会主导、 经济主导，逐步转向人类福利主导。如何在不断可能和可行的技术世界里坚守人类的基本价值观和伦理底线，甚至设定人类的伦理底线，是人类应当且必须具备的能力。在人工智能发展已经触及到人类价值观和伦理底线的时刻，如何守住底线，不只是一种坚守和价值观，更是一种把人类组织起来并形成共识的能力。
> 

## RAG漫谈

RAG主要解决了大模型在生成时缺乏对上下文和权威知识的考虑的问题。通过外挂知识库，使得在生成回答之前能够从外部数据源获取信息，从而避免提供虚假信息。

我认为对于垂直领域下，RAG的核心是数据，以及数据切分方式。

### 交互

#### 有效溯源

当我们设计完成一个RAG系统后，一般会提供基于参考数据的回答，以及列出参考数据。如果用户的目的是去以此去搜寻数据，扩充不同的数据源，这样的方式无可厚非。但是如果用户的意图是要得到答案并且需要答案是真实的，那么这样的方式就有问题，用户需要遍历参考数据才能确认当前回答是否真实以及确定答案的源头。因此我们在回答的时候最好附上答案是依据哪些参考数据生成的标志，这样便于用户快速定位溯源。

#### 用户是否真的在问他输入的问题？

大部分情况下，用户无法表达真实的询问意图，有一种情况是我们在输入问题的时候，会下意识的忽略一些先验信息，比如地理位置，时间偏好。举个例子：今天天气怎么样？这个问题大概率是在问我所在的地方的天气怎么样？缺失了地理位置的信息，大模型基于用户的原始输入无法给出准确的答案。大模型可以通过反问的方式，对缺失的实体进行补充。另一种情况是当前问题与用户实际问题有一些偏差，通过对当前问题进行推荐或者改写问题。

### 技术

#### chunk切分

chunk切分最好按数据的结构语义切分，但是现实中的数据没那么理想，因此我们需要对不同类型的数据进行分别处理。

对于文本数据，一般分为结构化数据和非结构化数据，这里的结构化是指数据可以通过一些工具轻易的将数据结构解构。

对于结构化数据，只需要解析数据结构，按结构拼接拆分即可，这部分是最容易处理的。

对于非结构化数据，一般可以从产品角度是否有一些特殊字段，关键字，总结出一些规律，使用正则表达式对数据进行切分，兜底策略就是采用固定长度的循环切分。(Langchain提供了函数)

chunk切分最好按数据的结构语义切分，采用固定长度，overlap的函数切分应为兜底策略。

### 召回和排序

多路召回：基于BM25召回，基于embedding的召回，以及其他策略的召回。

排序：我采用的是RRF，因为RRF是对位置排序，我对一些数据进行抽样分析，通过数据分析，得到了一个位置的过滤策略，作后处理。也可以使用reranker，比如cohere和bge。

### 其他技术的尝试

#### Query transformation

query transformation确实对于提升召回质量有一些提升，但是存在2个问题：

1. 如果多路召回中有关键字召回，需要平衡生成query的多路召回，不然关键字召回可能会主导召回结果，导致召回结果变差。
2. query transformation生成结果的不稳定性导致每次召回结果不一致。

#### Post-fusion

post-fusion解决的问题是当上下文太长，模型无法很好的理解上下文，因此循环遍历每个召回文档进行RAG。这个确实对整体结果的质量有提升，但是提升有限，反观其带来的问题：推理时间太长，消耗token过多，因此弃用了这个方法。

### 未来优化点

1. 有时候技术上很难解决的问题，换个思路反而可能会很好解决，比如把选择交给用户，通过交互的方式让用户选择，既减少了盲目猜测用户意图，也减少了技术上的实现难度，但能达到事半功倍的效果。
2. 分别针对召回测和排序测对领域Embedding微调。
3. 数据切分精细化，流程化。

## Sora

OpenAI引领了数据驱动的潮流。如果模型能够无线逼近于上限，那么模型就必须依赖于数据驱动，即学习足够多的真实有效的数据。GPT结合上下文来预测一个字符，Sora文生图是学习文本和图像的概率分布。从包含信息的角度来说，文本是一维数据，图像是三维甚至四维数据，视频是多帧图像，包含了帧于帧之间的联系，ChatGPT在文字这种一维数据上的都需要千亿模型参数，对于图像想要达到ChatGPT的效果，其数据量，模型大小，算力应该都是数量级的倍增，视频更甚。从数据的离散连续角度来说，文本是离散数据，文本被映射成token，而图像是连续数据，模型需要学习像素间的关联信息，且图像一般是三通道，如果是深度图像则是四通道，图像模型的计算量级比文本模型要高几个数量级。就目前，基于算力成本考量，我并不太担心Sora会有ChatGPT的威力，即使Sora效果现在很惊艳，但是这种图像视频的模型推理的代价比文本的高的多，不一定比雇一个人的代价低。

## Reference

1. [大模型的幻觉问题调研: LLM Hallucination Survey](https://zhuanlan.zhihu.com/p/642648601)
2. [https://alignmentsurvey.com/](https://alignmentsurvey.com/)
3. [https://openai.com/blog/introducing-superalignment](https://openai.com/blog/introducing-superalignment)
4. [https://openai.com/blog/our-approach-to-alignment-research](https://openai.com/blog/our-approach-to-alignment-research)
5. [“ChatGPT，你怎么看？”——与 ChatGPT 探讨 AIGC 对人类职业的影响](http://www.shehui.pku.edu.cn/upload/editor/file/20230704/20230704153604_1210.pdf)